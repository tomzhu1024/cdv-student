Virginia Eubanks, in the podcast discussing her work *Automating Inequality*, introduced a tendency of relying more on machine learning and algorithms to make decisions. The terrifying fact is that though people normally see "algorithms" as eternally rational and objective, the bias of algorithms can hardly be eliminated. According to Eubanks, rather than fairing out the existed inequality, this trend is more likely to reinforce the injustice. To me, I think the effects of allowing AI to make decisions for us can be evaluated from three facets.

First, the creator and maintainer of the algorithms. Whether the algorithm is created and put into use out of a benign or malign purpose. Whether the creator intentionally makes the algorithm to be biased to satiate a small group of people's interests. For instance, some corporates consciously condone the bias in their decision-making algorithms for the sake of their own benefits. If the creator or maintainer of the algorithms harbor some selfish motivations, then it cannot be ensured that the algorithms can fairing out the inequality.

Second, how the algorithm is created. It is true that algorithms will never lie. Nevertheless, the data that train the algorithms also play an critical role, especially for machine learning algorithms. If the training dataset is not unbiased, then the model will be biased and could inadvertently reinforce the social inequality when it is put into use. For example, the object recognition algorithms on automated vehicles highly rely on the training dataset. If one object does not appear frequently enough in the training data, then the success rate of algorithms detecting this object in real cases will decrease, causing potential risks. Another disadvantage of modern machine learning algorithms is that people cannot adjust or tweak the very details of a model. That is to say, when a model is trained, little can be done to intervene its decision making process.

Third, to what extent the authority adopts the idea of letting AI making decisions. Even if the algorithms yields a result that fails to fair out the inequality, the result does not directly take effect until people accept it and behave according to it. It reminds me of the China's recent action to defend coronavirus, the green/yellow/red code that indicates people's health status. Since the authority highly trusts this system, its effect on people's life is also high. In some cases, one's code accidentally turns yellow or red because of system error, his/her life will be significantly affected, as he/she cannot go to many public places without a green code.

